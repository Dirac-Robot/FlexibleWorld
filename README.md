# FlexibleWorld

**VLM-based Goal-Conditioned World Agent for Particle Simulation**

Vision-Language Model (Qwen2-VL)ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ ìì—°ì–´ goalì„ ì§ì ‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© World Agent.

## ğŸ¯ Overview

```
"ì…ìë“¤ì„ ì™¼ìª½ìœ¼ë¡œ ì´ë™ì‹œì¼œ" (Natural Language Goal)
              +
        [64x64 RGB Image]
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VLM Policy (Qwen2-VL)                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Vision-Language Model                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚   [Image Tokens]  +  [Goal Text Tokens]                   â”‚  â”‚
â”‚  â”‚              â†“                                            â”‚  â”‚
â”‚  â”‚      Transformer Layers (with LoRA)                       â”‚  â”‚
â”‚  â”‚              â†“                                            â”‚  â”‚
â”‚  â”‚         Hidden States                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Action Type   â”‚  â”‚ Action Params  â”‚  â”‚     Value      â”‚    â”‚
â”‚  â”‚   (8 types)    â”‚  â”‚  (x,y,val,r)   â”‚  â”‚   Estimate     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                   â†“                   â†“
      APPLY_FORCE         x=54, y=32           V(s,g)
                          value=2.0
```

## âœ¨ Features

- **Vision-Language Model**: Qwen2-VLë¡œ ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì§ì ‘ ì²˜ë¦¬
- **Natural Language Goals**: í•œêµ­ì–´/ì˜ì–´ ìì—°ì–´ ëª…ë ¹ ì§€ì›
- **LoRA Fine-tuning**: íš¨ìœ¨ì ì¸ í•™ìŠµ (5.7M trainable / 2.2B total)
- **Dense Rewards**: Goal ì§„í–‰ë„ ê¸°ë°˜ ì„¸ë°€í•œ ë³´ìƒ
- **Generalizable**: ë‹¤ì–‘í•œ í™˜ê²½ì— ì ìš© ê°€ëŠ¥í•œ êµ¬ì¡°
- **PPO Training**: ì‹¤ì œ ì‹œë®¬ë ˆì´í„°ì™€ ìƒí˜¸ì‘ìš©í•˜ë©° í•™ìŠµ

## ğŸ—ï¸ Architecture

### VLM Policy Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VLM Training Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Phase 1: Data Generation (vLLM)                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Goal DSL (structured)  â†’  vLLM (Qwen-72B)  â†’  Natural Language   â”‚    â”‚
â”‚   â”‚  "move_to(id=1, x>90)"      augmentation       "ì…ìë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ"   â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â†“                                       â”‚
â”‚   Phase 2: BC Pretrain (Optional)                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  [Image] + [Goal Text]  â†’  Qwen2-VL (LoRA)  â†’  Action             â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â†“                                       â”‚
â”‚   Phase 3: PPO Fine-tune (Real Simulator)                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚   â”‚  â”‚ Render  â”‚ â”€â”€â”€â†’ â”‚  VLM Policy  â”‚ â”€â”€â”€â†’ â”‚ Execute in Sim      â”‚  â”‚    â”‚
â”‚   â”‚  â”‚ Image   â”‚      â”‚  (action)    â”‚      â”‚ Dense Reward        â”‚  â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚   â”‚       â†‘                                            â”‚              â”‚    â”‚
â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PPO Update â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â”‚   Model: Qwen2-VL-2B-Instruct (LoRA: 5.7M trainable / 2.2B total)          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Action Space

```
ActionType (8 types):
â”œâ”€â”€ 0: NOOP           - ì•„ë¬´ ë™ì‘ ì—†ìŒ
â”œâ”€â”€ 1: ADD_PARTICLE   - ì…ì ì¶”ê°€
â”œâ”€â”€ 2: SET_PROPERTY   - ì…ì ì†ì„± ë³€ê²½
â”œâ”€â”€ 3: APPLY_HEAT     - ì—´ ì ìš©
â”œâ”€â”€ 4: APPLY_FORCE    - í˜ ì ìš© (ë°€ê¸°)
â”œâ”€â”€ 5: APPLY_ATTRACTION - ì¸ë ¥ ì ìš© (ë‹¹ê¸°ê¸°)
â”œâ”€â”€ 6: APPLY_REPULSION  - ì²™ë ¥ ì ìš©
â””â”€â”€ 7: STEP           - ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰

Action Vector (7-dim):
[action_type, target, x, y, value, radius, property_type]
```

### Dense Reward System

```python
# Goalì— ê°€ê¹Œì›Œì§€ë©´ positive, ë©€ì–´ì§€ë©´ negative
DirectionalPushGoal: dot(movement, direction) â†’ [-0.5, 0.5]
ClusterGoal:         spread_before - spread_after â†’ [-0.3, 0.3]
SpreadGoal:          spread_after - spread_before â†’ [-0.3, 0.3]
VibrateGoal:         velocity_magnitude â†’ [-0.3, 0.3]
Success:             1.0
```

## ğŸ“¦ Installation

```bash
# Environment setup
conda env create -f environment.yaml
conda activate world

# Required packages
pip install transformers peft accelerate
pip install qwen-vl-utils  # For Qwen2-VL

# For data generation
pip install vllm openai
```

## ğŸš€ Quick Start

### VLM Policy Training

```bash
# Fast test (10 PPO epochs)
python train_vlm.py vlm_fast

# Full training (BC + PPO)
python train_vlm.py vlm_full

# Long training (100 epochs)
python train_vlm.py vlm_long

# Use larger 7B model
python train_vlm.py vlm_full vlm_7b
```

### Data Generation (vLLM)

```bash
# 1. Start vLLM server
./scripts/start_vllm.sh qwen32b

# 2. Generate training data
python scripts/generate_training_data.py \
    --n-goals 2000 \
    --n-variations 5 \
    --output data/goal_commands.jsonl
```

### Inference

```bash
# Run trained VLM agent
python inference.py \
    --model checkpoints/vlm_best.pt \
    --goal "ì…ìë“¤ì„ ëª¨ì•„ì¤˜"
```

## ğŸ”§ Configuration

### VLM Training Presets

| Preset | Description |
|--------|-------------|
| `vlm_fast` | Quick test (10 PPO epochs, no BC) |
| `vlm_full` | Full training (BC 5ep + PPO 50ep) |
| `vlm_long` | Production (BC 10ep + PPO 100ep) |
| `vlm_7b` | Use Qwen2-VL-7B (larger model) |

### Key Parameters

```python
# Model
config.model.vlm.name = 'Qwen/Qwen2-VL-2B-Instruct'
config.model.vlm.use_lora = True
config.model.vlm.lora_r = 16
config.model.vlm.lora_alpha = 32

# Training
config.train.batch_size = 8  # Smaller for VLM
config.train.lr = 1e-4

# PPO
config.vlm.ppo_epochs = 50
config.vlm.rollout_steps = 256
config.rl.clip_ratio = 0.2
config.rl.gamma = 0.99
```

### Example Commands

```bash
# Custom training
python train_vlm.py vlm_full \
    train.lr=3e-4 \
    vlm.ppo_epochs=100 \
    model.vlm.lora_r=32

# Debug mode
python train_vlm.py vlm_fast debug

# Override VLM model
python train_vlm.py vlm_full model.vlm.name:=Qwen/Qwen2-VL-7B-Instruct
```

## ğŸ“ Directory Structure

```
FlexibleWorld/
â”œâ”€â”€ config.py                 # ato scope configuration
â”œâ”€â”€ train_vlm.py              # VLM Policy training â­
â”œâ”€â”€ inference.py              # Inference pipeline
â”œâ”€â”€ dataset.py                # Data loading
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ goal_world_model.py   # GoalConditionedWorldModel
â”‚   â”œâ”€â”€ backbone.py           # CLIP/DINOv2/LLM wrappers
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ simulator/
â”‚   â”œâ”€â”€ core.py               # ParticleSimulator
â”‚   â”œâ”€â”€ action_operator.py    # ActionOperator
â”‚   â”œâ”€â”€ goal_env.py           # GoalConditionedEnv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_training_data.py  # vLLM data generation
â”‚   â””â”€â”€ start_vllm.sh              # vLLM server script
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ goal_commands_v2.jsonl     # Generated training data
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ vlm_best.pt                # Best VLM checkpoint
â”‚   â””â”€â”€ vlm_final.pt               # Final VLM checkpoint
â”‚
â””â”€â”€ logs/
    â””â”€â”€ vlm_*.log                  # Training logs
```

## ğŸ® Supported Goals

Natural language goals in Korean/English:

| Goal Type | Examples |
|-----------|----------|
| Directional | "move left", "ì…ìë“¤ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ", "push up" |
| Clustering | "cluster together", "ê°€ìš´ë°ë¡œ ëª¨ì•„", "group particles" |
| Scattering | "scatter", "í©ì–´ì§€ê²Œ í•´", "spread apart" |
| Temperature | "heat up", "ê°€ì—´í•´", "make it vibrate" |
| Position | "move to corner", "ì¤‘ì•™ìœ¼ë¡œ" |

## ğŸ”¬ Usage Example

```python
from train_vlm import VLMPolicy
from simulator.goal_env import GoalConditionedEnv
from PIL import Image

# Create environment
env = GoalConditionedEnv(width=64, height=64)

# Load trained VLM
model = VLMPolicy.from_checkpoint('checkpoints/vlm_best.pt')

# Run episode
obs = env.reset()
goal = "ì…ìë“¤ì„ ì™¼ìª½ìœ¼ë¡œ ì´ë™ì‹œì¼œ"

for _ in range(100):
    # Render current state
    image = Image.fromarray(env.render())
    
    # Get action from VLM
    action, _, _ = model.sample_action([image], [goal])
    
    # Execute
    obs, reward, done, info = env.step(action[0].numpy())
    
    if done:
        break
```

## ğŸ“Š Performance

| Model | Success Rate | Trainable Params | Inference Speed |
|-------|-------------|------------------|-----------------|
| Qwen2-VL-2B | ~40% | 5.7M | ~15 FPS |
| Qwen2-VL-7B | TBD | ~10M | ~8 FPS |

## ğŸ—ºï¸ Roadmap

- [ ] Multi-GPU DDP training
- [ ] BC pretrain from expert demonstrations
- [ ] Transfer to other simulation environments
- [ ] Real robot deployment

## ğŸ“„ License

MIT
